{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Google Colab. Skipping repository cloning.\n",
      "Added /Users/giovanna/Desktop/federated-learning-project to Python path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def running_in_colab():\n",
    "    return 'google.colab' in sys.modules or os.path.exists('/content')\n",
    "\n",
    "branch = \"giovanna/centralized-baseline\"\n",
    "username = \"giovanna-brod-zamojska\"\n",
    "repo = \"federated-learning-project\"\n",
    "\n",
    "is_private = True\n",
    "\n",
    "def clone_repo_if_needed(exists_ok: bool, username: str, repository: str, is_private: bool, branch: str = None):\n",
    "\n",
    "  colab_repo_path = f'/content/{repository}/'\n",
    "  \n",
    "  if running_in_colab():\n",
    "\n",
    "    if exists_ok and os.path.exists(colab_repo_path):\n",
    "        print(f\"Repository already exists at {colab_repo_path}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(colab_repo_path) or not exists_ok:\n",
    "\n",
    "        # Remove any existing repo\n",
    "        print(f\"Removing content of {colab_repo_path}\")\n",
    "        os.system(f\"rm -rf {colab_repo_path}\")\n",
    "        print(\"Current directory files and folders:\", os.system(\"ls\"))\n",
    "\n",
    "        print(\"Cloning GitHub repo...\")\n",
    "\n",
    "        if is_private:\n",
    "            # Clone private repository\n",
    "            # Clone the GitHub repo (only needed once, if not already cloned)\n",
    "            from getpass import getpass\n",
    "\n",
    "\n",
    "            # Prompt for GitHub token (ensure token has access to the repo)\n",
    "            token = getpass('Enter GitHub token: ')\n",
    "\n",
    "            if branch:\n",
    "              !git clone --branch {branch} https://{username}:{token}@github.com/{username}/{repo}.git\n",
    "            else: \n",
    "              !git clone https://{username}:{token}@github.com/{username}/{repo}.git\n",
    "\n",
    "        else:\n",
    "            # Clone public repository\n",
    "            if branch:\n",
    "              !git clone --branch {branch} https://github.com/{username}/{repo}.git\n",
    "            else:\n",
    "              !git clone https://github.com/{username}/{repo}.git\n",
    "\n",
    "\n",
    "    requirements_path = f\"{colab_repo_path}/colab-requirements.txt\"\n",
    "    !pip install -r \"$requirements_path\"\n",
    "\n",
    "  else:\n",
    "    print(\"Not running in Google Colab. Skipping repository cloning.\")#\n",
    "\n",
    "\n",
    "\n",
    "def setup_notebook(repo_root_name: str = \"federated-learning-project\"):\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    if running_in_colab():\n",
    "        print(\"Sys.path: \", sys.path)\n",
    "\n",
    "        colab_repo_path = f'/content/{repo_root_name}/'\n",
    "         # Add the repository root to sys.path so modules can be imported\n",
    "        if str(colab_repo_path) not in sys.path:\n",
    "            sys.path.insert(0, colab_repo_path)\n",
    "            print(f\"Added {colab_repo_path} to sys.path\")\n",
    "    else:\n",
    "      \n",
    "        notebook_dir = Path().absolute()\n",
    "        project_root = notebook_dir.parent.parent\n",
    "\n",
    "        # Add project root to Python path if not already present\n",
    "        if str(project_root) not in sys.path:\n",
    "            sys.path.insert(0, str(project_root))\n",
    "            print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "        \n",
    "clone_repo_if_needed(branch=branch, exists_ok=True, username=username, repository=repo, is_private=is_private)\n",
    "\n",
    "setup_notebook()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting random seed to 42\n",
      "Dataset found at ./data. Loading...\n",
      "\n",
      "Running experiment 1/192 with config: {'batch_size': 32, 'lr': 0.2, 'weight_decay': 1e-05, 'momentum': 0.8, 'epochs': 5, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgiovannabrod\u001b[0m (\u001b[33mfederated-learning-team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb_logs/wandb/run-20250524_101228-2lxldjqg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/federated-learning-team/federated-learning-project/runs/2lxldjqg' target=\"_blank\">run_2025-05-24_baseline_config1</a></strong> to <a href='https://wandb.ai/federated-learning-team/federated-learning-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/federated-learning-team/federated-learning-project' target=\"_blank\">https://wandb.ai/federated-learning-team/federated-learning-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/federated-learning-team/federated-learning-project/runs/2lxldjqg' target=\"_blank\">https://wandb.ai/federated-learning-team/federated-learning-project/runs/2lxldjqg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/giovanna/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded default DINO ViT-S/16 model. Model architecture: VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Identity()\n",
      ")\n",
      "Using device: cpu\n",
      "Checkpoint directory: ./checkpoints\n",
      "Centralized Baseline Trainer initialized.\n",
      "Replaced model.head (Identity) with Linear(384, 100)\n",
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Linear(in_features=384, out_features=100, bias=True)\n",
      ")\n",
      "Training started\n",
      "Training. Start epoch: 1, End epoch: 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 76/1250 [00:54<07:16,  2.69batch/s, loss=0.182, f1_macro=0.00198, f1_micro=0.00905, accuracy=0.00905] "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from src.classes.trainer import Trainer as CentralizedBaselineTrainer\n",
    "from src.classes.cifar100_dataset import CIFAR100Dataset_v2 as CIFAR100Dataset\n",
    "from src.classes.experiment_manager import ExperimentManager\n",
    "from itertools import product\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "experiments_dir = \"./output\"\n",
    "\n",
    "if running_in_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    experiments_dir = \"/content/drive/MyDrive/polito2025/MLDL/Project/experiments\"\n",
    "    checkpoint_dir = experiments_dir + \"/checkpoints\"\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    print(f\"Setting random seed to {seed}\")\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def run_experiments(seed: int, resume: str = None):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Define hyperparameter search space dynamically\n",
    "    grid_dict = {\n",
    "        \"batch_size\": [32, 64, 128],\n",
    "        \"lr\": [0.1, 0.01, 0.001],\n",
    "        \"weight_decay\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "        \"momentum\": [0.8, 0.9, 0.95],\n",
    "        \"epochs\": [4],\n",
    "        \"seed\": [seed],\n",
    "    }\n",
    "\n",
    "    # Generate param grid from all combinations\n",
    "    keys, values = zip(*grid_dict.items())\n",
    "    param_grid = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    manager = ExperimentManager(\n",
    "        base_config={\n",
    "            \"seed\": seed,\n",
    "            \"lr\": 0.1,\n",
    "            \"momentum\": 0.9,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"epochs\": 30,\n",
    "            \"batch_size\": 64, \n",
    "            \"num_workers\": 4,\n",
    "        },\n",
    "        param_grid=param_grid,\n",
    "        use_wandb=True,\n",
    "        project_name=\"federated-learning-project\",\n",
    "        group_name=\"centralized-baseline\",\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "    )\n",
    "    _, _, results = manager.run(\n",
    "        trainer_class=CentralizedBaselineTrainer,\n",
    "        dataset=CIFAR100Dataset(),\n",
    "        run_name=\"baseline\",\n",
    "        run_tags=[\"test\", \"convergence\", \"v0\"],\n",
    "        resume=resume,\n",
    "    )\n",
    "    print(\"Experiments completed.\\n\")\n",
    "\n",
    "\n",
    "    filename = \"experiment_results_baseline_test_convergence_v2.json\"  \n",
    "\n",
    "    os.makedirs(experiments_dir, exist_ok=True)\n",
    "    file_path = os.path.join(experiments_dir, filename)\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"Results saved to {file_path}\")\n",
    "\n",
    "\n",
    "run_experiments(seed=42)\n",
    "# run_experiments(seed=42, resume=\"checkpoints/checkpoint.pth\")\n",
    "\n",
    "# comments:\n",
    "# higher batch sizes lead in general to a lower accuracy (128, 256) but a faster training time\n",
    "#  (256 faster than 128 which is much faster than 64)\n",
    "#  - 32 and 64 seems to be better \n",
    "\n",
    "# a learning rate of 0.01 is far better than 0.1, 0.2 (faster convergence) - but it can also lead to overfitting if not properly managed (the validation loss can start increasing - warning)\n",
    "# 0.01 > 0.2 > 0.1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
