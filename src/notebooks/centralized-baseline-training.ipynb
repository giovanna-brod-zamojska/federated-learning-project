{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Google Colab. Skipping repository cloning.\n",
      "Added /Users/giovanna/Desktop/federated-learning-project to Python path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def running_in_colab():\n",
    "    return 'google.colab' in sys.modules or os.path.exists('/content')\n",
    "\n",
    "branch = \"giovanna/centralized-baseline\"\n",
    "username = \"giovanna-brod-zamojska\"\n",
    "repo = \"federated-learning-project\"\n",
    "\n",
    "is_private = True\n",
    "\n",
    "def clone_repo_if_needed(exists_ok: bool, username: str, repository: str, is_private: bool, branch: str = None):\n",
    "\n",
    "  colab_repo_path = f'/content/{repository}/'\n",
    "  \n",
    "  if running_in_colab():\n",
    "\n",
    "    if exists_ok and os.path.exists(colab_repo_path):\n",
    "        print(f\"Repository already exists at {colab_repo_path}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(colab_repo_path) or not exists_ok:\n",
    "\n",
    "        # Remove any existing repo\n",
    "        print(f\"Removing content of {colab_repo_path}\")\n",
    "        os.system(f\"rm -rf {colab_repo_path}\")\n",
    "        print(\"Current directory files and folders:\", os.system(\"ls\"))\n",
    "\n",
    "        print(\"Cloning GitHub repo...\")\n",
    "\n",
    "        if is_private:\n",
    "            # Clone private repository\n",
    "            # Clone the GitHub repo (only needed once, if not already cloned)\n",
    "            from getpass import getpass\n",
    "\n",
    "\n",
    "            # Prompt for GitHub token (ensure token has access to the repo)\n",
    "            token = getpass('Enter GitHub token: ')\n",
    "\n",
    "            if branch:\n",
    "              !git clone --branch {branch} https://{username}:{token}@github.com/{username}/{repo}.git\n",
    "            else: \n",
    "              !git clone https://{username}:{token}@github.com/{username}/{repo}.git\n",
    "\n",
    "        else:\n",
    "            # Clone public repository\n",
    "            if branch:\n",
    "              !git clone --branch {branch} https://github.com/{username}/{repo}.git\n",
    "            else:\n",
    "              !git clone https://github.com/{username}/{repo}.git\n",
    "\n",
    "\n",
    "    requirements_path = f\"{colab_repo_path}/colab-requirements.txt\"\n",
    "    !pip install -r \"$requirements_path\"\n",
    "\n",
    "  else:\n",
    "    print(\"Not running in Google Colab. Skipping repository cloning.\")#\n",
    "\n",
    "\n",
    "\n",
    "def setup_notebook(repo_root_name: str = \"federated-learning-project\"):\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    if running_in_colab():\n",
    "        print(\"Sys.path: \", sys.path)\n",
    "\n",
    "        colab_repo_path = f'/content/{repo_root_name}/'\n",
    "         # Add the repository root to sys.path so modules can be imported\n",
    "        if str(colab_repo_path) not in sys.path:\n",
    "            sys.path.insert(0, colab_repo_path)\n",
    "            print(f\"Added {colab_repo_path} to sys.path\")\n",
    "    else:\n",
    "      \n",
    "        notebook_dir = Path().absolute()\n",
    "        project_root = notebook_dir.parent.parent\n",
    "\n",
    "        # Add project root to Python path if not already present\n",
    "        if str(project_root) not in sys.path:\n",
    "            sys.path.insert(0, str(project_root))\n",
    "            print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "        \n",
    "clone_repo_if_needed(branch=branch, exists_ok=True, username=username, repository=repo, is_private=is_private)\n",
    "\n",
    "setup_notebook()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting random seed to 42\n",
      "{'batch_size': 128, 'lr': 0.1, 'weight_decay': 0.0005, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4, 'accum_steps': 6, 'optimizer_type': 'SGD', 'augment': None}\n",
      "Dataset found at ./data. Loading...\n",
      "\n",
      "Running experiment 1/12 with config: {'batch_size': 128, 'lr': 0.1, 'weight_decay': 0.0005, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4, 'accum_steps': 6, 'optimizer_type': 'SGD', 'augment': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgiovannabrod\u001b[0m (\u001b[33mfederated-learning-team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb_logs/wandb/run-20250601_001314-3e99m3qi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/federated-learning-team/federated-learning-project/runs/3e99m3qi' target=\"_blank\">run_baseline_batch_size128-lr0.1-weight_decay0.0005-momentum0.9-epochs5-seed42-num_workers4-accum_steps6-optimizer_typeSGD-augmentNone</a></strong> to <a href='https://wandb.ai/federated-learning-team/federated-learning-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/federated-learning-team/federated-learning-project' target=\"_blank\">https://wandb.ai/federated-learning-team/federated-learning-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/federated-learning-team/federated-learning-project/runs/3e99m3qi' target=\"_blank\">https://wandb.ai/federated-learning-team/federated-learning-project/runs/3e99m3qi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/giovanna/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINO ViT-S/16 model instantied. Using device: cpu\n",
      "Replaced model.head (Identity) with Linear(384, 100)\n",
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Linear(in_features=384, out_features=100, bias=True)\n",
      ")\n",
      "ðŸ”’ Encoder parameters frozen, only head parameters are trainable.\n",
      "Checkpoint directory: ./checkpoints\n",
      "Trainer initialized.\n",
      "Training started\n",
      "Training. Start epoch: 1, End epoch: 5.\n",
      "\n",
      "EPOCH 1\n",
      "Using Learning Rate = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/313 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from src.classes.trainer import Trainer as CentralizedBaselineTrainer\n",
    "from src.classes.dataset import CIFAR100Dataset\n",
    "from src.classes.experiment_manager import ExperimentManager\n",
    "from itertools import product\n",
    "\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "experiments_dir = \"./output\"\n",
    "\n",
    "if running_in_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    experiments_dir = \"/content/drive/MyDrive/polito2025/MLDL/Project/experiments\" # define your Google Drive path here\n",
    "    checkpoint_dir = experiments_dir + \"/checkpoints\"\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    print(f\"Setting random seed to {seed}\")\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def run_experiments(seed: int):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    exp = \"14\"\n",
    "\n",
    "    grid_dict = {\n",
    "        \"batch_size\": [128],\n",
    "        \"lr\": [0.1, 0.05, 0.01, 0.001],\n",
    "        \"weight_decay\": [5e-4],\n",
    "        \"momentum\": [0.9],\n",
    "        \"epochs\": [5, 10, 20],\n",
    "        \"seed\": [seed],\n",
    "        \"num_workers\": [4],\n",
    "        \"accum_steps\": [6],\n",
    "        \"optimizer_type\": [\"SGD\"],\n",
    "        \"augment\": [None]\n",
    "    }\n",
    "\n",
    "    # Generate param grid from all combinations\n",
    "    keys, values = zip(*grid_dict.items())\n",
    "    param_grid = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "    manager = ExperimentManager(\n",
    "        param_grid=param_grid,\n",
    "        use_wandb=True,\n",
    "        project_name=\"federated-learning-project\", #wandb\n",
    "        group_name=\"centralized-baseline\", #wandb\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "    )\n",
    "    _, _, results = manager.run(\n",
    "        trainer_class=CentralizedBaselineTrainer,\n",
    "        dataset_class=CIFAR100Dataset,\n",
    "        run_name=\"baseline\", #wandb\n",
    "        run_tags=[\"full-training\", f\"v{exp}\"], #wandb\n",
    "        resume_training_from_config=None\n",
    "    )\n",
    "    print(\"Experiments completed.\\n\")\n",
    "\n",
    "\n",
    "    # filename = f\"experiment_baseline_v{exp}_bs{bs}_lr{lr}_Tmax{Tmax}_ep{ep}_accum_steps{accum_steps}.json\"  \n",
    "    filename = f\"experiment_baseline_full_param_grid_search_v{exp}.json\"  \n",
    "    os.makedirs(experiments_dir, exist_ok=True)\n",
    "    file_path = os.path.join(experiments_dir, filename)\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"Results saved to {file_path}\")\n",
    "\n",
    "try:\n",
    "    run_experiments(seed=42)\n",
    "except:\n",
    "    import traceback\n",
    "    print(traceback.format_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
