{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def running_in_colab():\n",
    "    return 'google.colab' in sys.modules or os.path.exists('/content')\n",
    "\n",
    "branch = \"main\"\n",
    "username = \"giovanna-brod-zamojska\"\n",
    "repo = \"federated-learning-project\"\n",
    "\n",
    "is_private = True\n",
    "\n",
    "def clone_repo_if_needed(exists_ok: bool, username: str, repository: str, is_private: bool, branch: str = None):\n",
    "\n",
    "  colab_repo_path = f'/content/{repository}/'\n",
    "  \n",
    "  if running_in_colab():\n",
    "\n",
    "    if exists_ok and os.path.exists(colab_repo_path):\n",
    "        print(f\"Repository already exists at {colab_repo_path}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(colab_repo_path) or not exists_ok:\n",
    "\n",
    "        # Remove any existing repo\n",
    "        print(f\"Removing content of {colab_repo_path}\")\n",
    "        os.system(f\"rm -rf {colab_repo_path}\")\n",
    "        print(\"Current directory files and folders:\", os.system(\"ls\"))\n",
    "\n",
    "        print(\"Cloning GitHub repo...\")\n",
    "\n",
    "        if is_private:\n",
    "            # Clone private repository\n",
    "            # Clone the GitHub repo (only needed once, if not already cloned)\n",
    "            from getpass import getpass\n",
    "\n",
    "\n",
    "            # Prompt for GitHub token (ensure token has access to the repo)\n",
    "            token = getpass('Enter GitHub token: ')\n",
    "\n",
    "            if branch:\n",
    "              !git clone --branch {branch} https://{username}:{token}@github.com/{username}/{repo}.git\n",
    "            else: \n",
    "              !git clone https://{username}:{token}@github.com/{username}/{repo}.git\n",
    "\n",
    "        else:\n",
    "            # Clone public repository\n",
    "            if branch:\n",
    "              !git clone --branch {branch} https://github.com/{username}/{repo}.git\n",
    "            else:\n",
    "              !git clone https://github.com/{username}/{repo}.git\n",
    "\n",
    "\n",
    "    requirements_path = f\"{colab_repo_path}/colab-requirements.txt\"\n",
    "    !pip install -r \"$requirements_path\"\n",
    "\n",
    "  else:\n",
    "    print(\"Not running in Google Colab. Skipping repository cloning.\")#\n",
    "\n",
    "\n",
    "\n",
    "def setup_notebook(repo_root_name: str = \"federated-learning-project\"):\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    if running_in_colab():\n",
    "        print(\"Sys.path: \", sys.path)\n",
    "\n",
    "        colab_repo_path = f'/content/{repo_root_name}/'\n",
    "         # Add the repository root to sys.path so modules can be imported\n",
    "        if str(colab_repo_path) not in sys.path:\n",
    "            sys.path.insert(0, colab_repo_path)\n",
    "            print(f\"Added {colab_repo_path} to sys.path\")\n",
    "    else:\n",
    "      \n",
    "        notebook_dir = Path().absolute()\n",
    "        project_root = notebook_dir.parent.parent\n",
    "\n",
    "        # Add project root to Python path if not already present\n",
    "        if str(project_root) not in sys.path:\n",
    "            sys.path.insert(0, str(project_root))\n",
    "            print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "        \n",
    "clone_repo_if_needed(branch=branch, exists_ok=True, username=username, repository=repo, is_private=is_private)\n",
    "\n",
    "setup_notebook()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting random seed to 42\n",
      "Dataset found at ./data. Loading the dataset...\n",
      "\n",
      "Running experiment 1/18 with config: {'batch_size': 64, 'seed': 42, 'epochs': 30, 'lr': 0.1, 'weight_decay': 0.0001}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb_logs/wandb/run-20250517_104825-7cbntfxe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/federated-learning-team/federated-learning-project/runs/7cbntfxe' target=\"_blank\">run_run__1</a></strong> to <a href='https://wandb.ai/federated-learning-team/federated-learning-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/federated-learning-team/federated-learning-project' target=\"_blank\">https://wandb.ai/federated-learning-team/federated-learning-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/federated-learning-team/federated-learning-project/runs/7cbntfxe' target=\"_blank\">https://wandb.ai/federated-learning-team/federated-learning-project/runs/7cbntfxe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/giovanna/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     64\u001b[39m     manager.run(\n\u001b[32m     65\u001b[39m         trainer_class=CentralizedBaselineTrainer,\n\u001b[32m     66\u001b[39m         dataset=CIFAR100Dataset(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m         resume=resume,\n\u001b[32m     70\u001b[39m     )\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExperiments completed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# run_experiments(seed=42, resume=\"checkpoints/checkpoint.pth\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mrun_experiments\u001b[39m\u001b[34m(seed, resume)\u001b[39m\n\u001b[32m     23\u001b[39m set_seed(seed)\n\u001b[32m     25\u001b[39m manager = ExperimentManager(\n\u001b[32m     26\u001b[39m     base_config={\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     group_name=\u001b[33m\"\u001b[39m\u001b[33mcentralized-baseline\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     63\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCentralizedBaselineTrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCIFAR100Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_tags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExperiments completed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/federated-learning-project/src/classes/experiment_manager.py:116\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(self, trainer_class, dataset, run_name, run_tags, resume, metric_for_best_config)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    105\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.param_grid)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m )\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_wandb:\n\u001b[32m    109\u001b[39m     run = wandb.init(\n\u001b[32m    110\u001b[39m         project=\u001b[38;5;28mself\u001b[39m.project_name,\n\u001b[32m    111\u001b[39m         group=\u001b[38;5;28mself\u001b[39m.group_name,  \u001b[38;5;66;03m# Group runs under this name\u001b[39;00m\n\u001b[32m    112\u001b[39m         name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_config\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Name of the run\u001b[39;00m\n\u001b[32m    113\u001b[39m         notes=notes,\n\u001b[32m    114\u001b[39m         config=config,\n\u001b[32m    115\u001b[39m         tags=run_tags,\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28mdir\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33m./wandb_logs\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Directory to save logs\u001b[39;00m\n\u001b[32m    117\u001b[39m         reinit=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Reinitialize WandB for each run\u001b[39;00m\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    120\u001b[39m _, train_loader, val_loader, test_loader = \u001b[38;5;28mself\u001b[39m.setup_dataset(\n\u001b[32m    121\u001b[39m     dataset, config\n\u001b[32m    122\u001b[39m )\n\u001b[32m    124\u001b[39m trainer = trainer_class(\n\u001b[32m    125\u001b[39m     **config,\n\u001b[32m    126\u001b[39m     num_classes=dataset.get_num_labels(),\n\u001b[32m    127\u001b[39m     use_wandb=\u001b[38;5;28mself\u001b[39m.use_wandb,\n\u001b[32m    128\u001b[39m     metric_for_best_model=metric_for_best_model,\n\u001b[32m    129\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/federated-learning-project/src/classes/trainer.py:293\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, num_classes, loss_fn, scheduler_type, epochs, use_wandb, metric_for_best_model, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    284\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    285\u001b[39m     num_classes: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m     **kwargs,\n\u001b[32m    292\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebookresearch/dino:main\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdino_vits16\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    295\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/federated-learning-project/venv/lib/python3.11/site-packages/torch/hub.py:566\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source == \u001b[33m'\u001b[39m\u001b[33mgithub\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    563\u001b[39m     repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[33m\"\u001b[39m\u001b[33mload\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    564\u001b[39m                                        verbose=verbose, skip_validation=skip_validation)\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m model = \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/federated-learning-project/venv/lib/python3.11/site-packages/torch/hub.py:595\u001b[39m, in \u001b[36m_load_local\u001b[39m\u001b[34m(hubconf_dir, model, *args, **kwargs)\u001b[39m\n\u001b[32m    592\u001b[39m     hub_module = _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[32m    594\u001b[39m     entry = _load_entry_from_hubconf(hub_module, model)\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m     model = \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_dino_main/hubconf.py:27\u001b[39m, in \u001b[36mdino_vits16\u001b[39m\u001b[34m(pretrained, **kwargs)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdino_vits16\u001b[39m(pretrained=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs):\n\u001b[32m     23\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03m    ViT-Small/16x16 pre-trained with DINO.\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    Achieves 74.5% top-1 accuracy on ImageNet with k-NN classification.\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     model = \u001b[43mvits\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvit_small\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[32m     29\u001b[39m         state_dict = torch.hub.load_state_dict_from_url(\n\u001b[32m     30\u001b[39m             url=\u001b[33m\"\u001b[39m\u001b[33mhttps://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m             map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_dino_main/vision_transformer.py:244\u001b[39m, in \u001b[36mvit_small\u001b[39m\u001b[34m(patch_size, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvit_small\u001b[39m(patch_size=\u001b[32m16\u001b[39m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     model = \u001b[43mVisionTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLayerNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_dino_main/vision_transformer.py:163\u001b[39m, in \u001b[36mVisionTransformer.__init__\u001b[39m\u001b[34m(self, img_size, patch_size, in_chans, num_classes, embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_scale, drop_rate, attn_drop_rate, drop_path_rate, norm_layer, **kwargs)\u001b[39m\n\u001b[32m    161\u001b[39m trunc_normal_(\u001b[38;5;28mself\u001b[39m.pos_embed, std=\u001b[32m.02\u001b[39m)\n\u001b[32m    162\u001b[39m trunc_normal_(\u001b[38;5;28mself\u001b[39m.cls_token, std=\u001b[32m.02\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/federated-learning-project/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:890\u001b[39m, in \u001b[36mModule.apply\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[32m    855\u001b[39m \n\u001b[32m    856\u001b[39m \u001b[33;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m \n\u001b[32m    888\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m890\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/federated-learning-project/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:890\u001b[39m, in \u001b[36mModule.apply\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[32m    855\u001b[39m \n\u001b[32m    856\u001b[39m \u001b[33;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m \n\u001b[32m    888\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m890\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "    \u001b[31m[... skipping similar frames: Module.apply at line 890 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/federated-learning-project/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:890\u001b[39m, in \u001b[36mModule.apply\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\u001b[39;00m\n\u001b[32m    855\u001b[39m \n\u001b[32m    856\u001b[39m \u001b[33;03mTypical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m \n\u001b[32m    888\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m890\u001b[39m     \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/federated-learning-project/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:891\u001b[39m, in \u001b[36mModule.apply\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m    890\u001b[39m     module.apply(fn)\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_dino_main/vision_transformer.py:167\u001b[39m, in \u001b[36mVisionTransformer._init_weights\u001b[39m\u001b[34m(self, m)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, m):\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn.Linear):\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m         \u001b[43mtrunc_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m.02\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, nn.Linear) \u001b[38;5;129;01mand\u001b[39;00m m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    169\u001b[39m             nn.init.constant_(m.bias, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_dino_main/utils.py:550\u001b[39m, in \u001b[36mtrunc_normal_\u001b[39m\u001b[34m(tensor, mean, std, a, b)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrunc_normal_\u001b[39m(tensor, mean=\u001b[32m0.\u001b[39m, std=\u001b[32m1.\u001b[39m, a=-\u001b[32m2.\u001b[39m, b=\u001b[32m2.\u001b[39m):\n\u001b[32m    549\u001b[39m     \u001b[38;5;66;03m# type: (Tensor, float, float, float, float) -> Tensor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_trunc_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_dino_main/utils.py:537\u001b[39m, in \u001b[36m_no_grad_trunc_normal_\u001b[39m\u001b[34m(tensor, mean, std, a, b)\u001b[39m\n\u001b[32m    533\u001b[39m tensor.uniform_(\u001b[32m2\u001b[39m * l - \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m * u - \u001b[32m1\u001b[39m)\n\u001b[32m    535\u001b[39m \u001b[38;5;66;03m# Use inverse cdf transform for normal distribution to get truncated\u001b[39;00m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m# standard normal\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43merfinv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# Transform to proper mean, std\u001b[39;00m\n\u001b[32m    540\u001b[39m tensor.mul_(std * math.sqrt(\u001b[32m2.\u001b[39m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from src.classes.trainer import Trainer as CentralizedBaselineTrainer\n",
    "from src.classes.cifar100_dataset import CIFAR100Dataset\n",
    "from src.classes.experiment_manager import ExperimentManager\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    print(f\"Setting random seed to {seed}\")\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def run_experiments(seed: int, resume: str = None):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    manager = ExperimentManager(\n",
    "        base_config={\n",
    "            \"seed\": seed,\n",
    "            \"lr\": 0.1, # [0.01, 0.2]\n",
    "            \"momentum\": 0.9, # [0.8, 0.99]\n",
    "            \"weight_decay\": 1e-4, # [1e-5, 1e-3]\n",
    "            \"epochs\": 30, # [30, 100]\n",
    "            \"batch_size\": 64, # 64, 128, 256\n",
    "            \"num_workers\": 4,\n",
    "        },\n",
    "        param_grid=[\n",
    "            {\"batch_size\": 64, \"seed\": seed, \"epochs\": 30, \"lr\": 0.1, \"weight_decay\": 1e-4},\n",
    "            {\"batch_size\": 128, \"seed\": seed, \"epochs\": 30, \"lr\": 0.1, \"weight_decay\": 1e-4},\n",
    "            {\"batch_size\": 256, \"seed\": seed, \"epochs\": 30, \"lr\": 0.1, \"weight_decay\": 1e-4},\n",
    "\n",
    "            {\"batch_size\": 64, \"seed\": seed, \"epochs\": 30, \"lr\": 0.2, \"weight_decay\": 1e-4},\n",
    "            {\"batch_size\": 128, \"seed\": seed, \"epochs\": 30, \"lr\": 0.2, \"weight_decay\": 1e-4},\n",
    "            {\"batch_size\": 256, \"seed\": seed, \"epochs\": 30, \"lr\": 0.2, \"weight_decay\": 1e-4},\n",
    "\n",
    "            {\"batch_size\": 64, \"seed\": seed, \"epochs\": 30, \"lr\": 0.01, \"weight_decay\": 1e-4},\n",
    "            {\"batch_size\": 128, \"seed\": seed, \"epochs\": 30, \"lr\": 0.01, \"weight_decay\": 1e-4},\n",
    "            {\"batch_size\": 256, \"seed\": seed, \"epochs\": 30, \"lr\": 0.01, \"weight_decay\": 1e-4},\n",
    "\n",
    "            {\"batch_size\": 64, \"seed\": seed, \"epochs\": 30, \"lr\": 0.1, \"weight_decay\": 1e-3},\n",
    "            {\"batch_size\": 128, \"seed\": seed, \"epochs\": 30, \"lr\": 0.1, \"weight_decay\": 1e-3},\n",
    "            {\"batch_size\": 256, \"seed\": seed, \"epochs\": 30, \"lr\": 0.1, \"weight_decay\": 1e-3},\n",
    "\n",
    "            {\"batch_size\": 64, \"seed\": seed, \"epochs\": 30, \"lr\": 0.2, \"weight_decay\": 1e-3},\n",
    "            {\"batch_size\": 128, \"seed\": seed, \"epochs\": 30, \"lr\": 0.2, \"weight_decay\": 1e-3},\n",
    "            {\"batch_size\": 256, \"seed\": seed, \"epochs\": 30, \"lr\": 0.2, \"weight_decay\": 1e-3},\n",
    "\n",
    "            {\"batch_size\": 64, \"seed\": seed, \"epochs\": 30, \"lr\": 0.01, \"weight_decay\": 1e-3},\n",
    "            {\"batch_size\": 128, \"seed\": seed, \"epochs\": 30, \"lr\": 0.01, \"weight_decay\": 1e-3},\n",
    "            {\"batch_size\": 256, \"seed\": seed, \"epochs\": 30, \"lr\": 0.01, \"weight_decay\": 1e-3},\n",
    "        ],\n",
    "        use_wandb=True,\n",
    "        project_name=\"federated-learning-project\",\n",
    "        group_name=\"centralized-baseline\",\n",
    "    )\n",
    "    manager.run(\n",
    "        trainer_class=CentralizedBaselineTrainer,\n",
    "        dataset=CIFAR100Dataset(),\n",
    "        run_name=\"run_\",\n",
    "        run_tags=[\"test\"],\n",
    "        resume=resume,\n",
    "    )\n",
    "    print(\"Experiments completed.\\n\")\n",
    "\n",
    "run_experiments(seed=42)\n",
    "# run_experiments(seed=42, resume=\"checkpoints/checkpoint.pth\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
