a first experiment was carried out to evaluate how different parameters influence the leanring curve.
the experiments were carried out at a fixed seed and a small number of epochs (5).

(V8)

purpose: effect of momentum and weight decay at different learning rates, given a fixed batch size

batch_size fixed = 128, 
momentum = [0.8, 0.9]
wd = [0.0005, 0, 0.001, 0.0001]
lr = [0.001, 0.005, 0.01]

Results:
src/notebooks/experiments_results/centralized-baseline/experiment_results_baseline_test_convergence_v8.json

we noticed that , for all learning rates:

weight decay: little or no effect - therefore we choose to fix it to 5e-4, which is not the best for all learning rates but the difference in accuracy is very small by changing the weight decay

momentum: small effect but visible - best momentum 0.9 for all lr values



(V14)

and we carry out the final experiments by further varying the lr, and also varying the epochs (too see how much gain in accuracy we have bychoosing a higher number of epochs)

results:src/notebooks/experiments_results/centralized-baseline/experiment_baseline_full_param_grid_search_v14.json

best config:

{
        "config": {
            "batch_size": 128,
            "lr": 0.01,
            "weight_decay": 0.0005,
            "momentum": 0.9,
            "epochs": 20,
            "seed": 42,
            "num_workers": 4,
            "accum_steps": 1,
            "optimizer_type": "SGD",
            "augment": null
        },
        "val_metric": 0.7458999752998352
    },
Test accuracy = 74.62%
----

Notice that the same setting with epoch=10 leads to slightly lower accuracy (the difference isnt that big):

    {
        "config": {
            "batch_size": 128,
            "lr": 0.01,
            "weight_decay": 0.0005,
            "momentum": 0.9,
            "epochs": 10,
            "seed": 42,
            "num_workers": 4,
            "accum_steps": 1,
            "optimizer_type": "SGD",
            "augment": null
        },
        "val_metric": 0.7418000102043152
    }, 
    test accuracy = 74.79%

------

Notice.

Our results maybe do not outperform the literature (search  papers on performances reached by other people with vits16 on cifar100)

but it should also be considered that we apply simple transformations (mention it):

 self.train_transform = transforms.Compose(
            [
                transforms.RandomResizedCrop(224),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
                ),
            ]
        )
A part from resizing and normalizing which is needed, we apply only RandomHorziontalFlip to augment the dataset and reduce the possibility of overfitting it by having moer varying data.
no rand augment, no mixup or other more complicated processing techniques which can improve performance

(WE didnt apply such other complicated technqieu becaue of the amount of overhead they provided when iterating over the dataloader)
