
# Running for few epochs in order to understand how the hyperparameters choice influence the learning training curve and corresponding val accuracies
# Experiment v8 (google colab T4 GPU): 

effect of weight decay: small/null (experiments with the same params that only differs in the weight decay have overlapping learning training curves and very similar validation accuracies)
--> lets choose a weight decay usually used and stick on it 
effect of momentum: 0.9 better than 0.8 
--> lets set momentum= 0.9 or eventually experiment with 0.95
effect of the learning rate: it is the one influencing more the learning curve and the validation accuracy
--> experiment more with this parameters, notice: learning rate depends on batch size, higher batch size needs a higher lr (found a github comment on this but we need to find some papers demonstrating it, or we demostrate it thorugh our experiments)


# 8 6 2 4 
# Running experiment 8/24 with config: {'batch_size': 128, 'lr': 0.01, 'weight_decay': 0.001, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 6/24 with config: {'batch_size': 128, 'lr': 0.01, 'weight_decay': 0.0005, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 2/24 with config: {'batch_size': 128, 'lr': 0.01, 'weight_decay': 0, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 4/24 with config: {'batch_size': 128, 'lr': 0.01, 'weight_decay': 0.0001, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}

# 7 1 5 3
# Running experiment 7/24 with config: {'batch_size': 128, 'lr': 0.01, 'weight_decay': 0.001, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 1/24 with config: {'batch_size': 128, 'lr': 0.01, 'weight_decay': 0, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 5/24 with config: {'batch_size': 128, 'lr': 0.01, 'weight_decay': 0.0005, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 3/24 with config: {'batch_size': 128, 'lr': 0.01, 'weight_decay': 0.0001, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}

# 10 12 14 16
# Running experiment 10/24 with config: {'batch_size': 128, 'lr': 0.005, 'weight_decay': 0, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 12/24 with config: {'batch_size': 128, 'lr': 0.005, 'weight_decay': 0.0001, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 14/24 with config: {'batch_size': 128, 'lr': 0.005, 'weight_decay': 0.0005, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 16/24 with config: {'batch_size': 128, 'lr': 0.005, 'weight_decay': 0.001, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}

# 11 15 9 13 
# Running experiment 11/24 with config: {'batch_size': 128, 'lr': 0.005, 'weight_decay': 0.0001, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 15/24 with config: {'batch_size': 128, 'lr': 0.005, 'weight_decay': 0.001, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 9/24 with config: {'batch_size': 128, 'lr': 0.005, 'weight_decay': 0, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 13/24 with config: {'batch_size': 128, 'lr': 0.005, 'weight_decay': 0.0005, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}

# 18 20 22 24 --- SIGNIFICANTLY SLOWER TRAINING CURVE AND VALIDATION ACCURACY : too much low learning rate! 
# Running experiment 18/24 with config: {'batch_size': 128, 'lr': 0.001, 'weight_decay': 0, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 20/24 with config: {'batch_size': 128, 'lr': 0.001, 'weight_decay': 0.0001, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 22/24 with config: {'batch_size': 128, 'lr': 0.001, 'weight_decay': 0.0005, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 24/24 with config: {'batch_size': 128, 'lr': 0.001, 'weight_decay': 0.001, 'momentum': 0.9, 'epochs': 5, 'seed': 42, 'num_workers': 4}

# 17 19 21 23 -- even more SIGNIFICANTLY SLOWER TRAINING CURVE AND VALIDATION ACCURACY : too much low learning rate + low momentum! 
# Running experiment 17/24 with config: {'batch_size': 128, 'lr': 0.001, 'weight_decay': 0, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 19/24 with config: {'batch_size': 128, 'lr': 0.001, 'weight_decay': 0.0001, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 21/24 with config: {'batch_size': 128, 'lr': 0.001, 'weight_decay': 0.0005, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}
# Running experiment 23/24 with config: {'batch_size': 128, 'lr': 0.001, 'weight_decay': 0.001, 'momentum': 0.8, 'epochs': 5, 'seed': 42, 'num_workers': 4}

# ----------------------------------

üèÜBest config: {
    "batch_size": 128,
    "lr": 0.01,
    "weight_decay": 0.0005,
    "momentum": 0.9,
    "epochs": 5,
    "seed": 42,
    "num_workers": 4
} with validation accuracy: 73.57%