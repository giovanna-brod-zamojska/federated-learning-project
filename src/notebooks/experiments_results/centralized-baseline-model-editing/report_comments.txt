
##### context #####

our fisher information matrix is not computad as it should be.
We are accumulating the fisher information using batches instead of computing gradients per each sampled.


Question to ChatGPT to justify this choice. Response ChatGPT:
----------------------------------------------------
âœ… Justification of the Approximated Fisher
Your method approximates the Fisher Information Matrix (FIM) by averaging squared gradients of the loss over batches:
scores[name] += param.grad.detach() ** 2
This is a common and accepted approximation used in the original Elastic Weight Consolidation (EWC) paper by Kirkpatrick et al. (2017),
Instead of sampling from the true posterior, the approximation uses the empirical average of the squared gradients over the dataset.
Original EWC Paper:
Kirkpatrick et al. (2017) - Overcoming catastrophic forgetting in neural networks
They use the diagonal of the empirical Fisher, computed as the average of squared gradients.
--------------- Warning: this should be verified -----------


We then follow the approach specified in TA paper, where the mask is not computed in one shot (1 round), 
but it is calibrated acrross multiple rounds.
So we implemented the algortyhm of TaLoS they wrote in the paper.
(see: https://arxiv.org/abs/2504.02620)


##### experiments #####

experiment 1:

run sparse finetuning with mask calibration using feshear least important on the best centralized baseline setting:

RECALL
centralized-baseline-best hyperparams:
{
        "config": {
            "batch_size": 128,
            "lr": 0.01,
            "weight_decay": 0.0005,
            "momentum": 0.9,
            "epochs": 20,
            "seed": 42,
            "num_workers": 4,
            "accum_steps": 1,
            "optimizer_type": "SGD",
            "augment": null
        },
        "val_metric": 0.7458999752998352
    },

    -----
We used this heperparameters , 
but 
- replaced SGD -> SparseSGD 
- applied fisher leat
- tested sparsity: [0.99, 0.95] 
- round 1

in order to understand the learning curve behavior.

Experiment 1:

Final setting applied:

grid_dict = {
    "batch_size": [128],
    "lr": [0.01],
    "weight_decay": [5e-4],
    "momentum": [0.9],
    "epochs": [20],
    "seed": [seed],
    "num_workers": [4],
    "accum_steps": [1],
    "optimizer_type": ["SparseSGD"], <--
    "augment": [None],
    "sparsity": [0.99, 0.95], <---
    "rounds": [1], #
    "num_batches": [None], 
    "strategy": ["train_least_important"], <---
    "approximate_fisher": [True], <---
}
Results:

See plot: src/notebooks/experiments_results/centralized-baseline-model-editing/plots/baseline_model_editing_exp1_20epochs.png

When applying a sparsity of 0.99 
(and therefore keeping only 1% of the least important fisher parameters [calculated 
according our approximated version of the fisher scores computation, which follows EWC ] )
the final validaiton accuracy slightly outperform the centralized baseline by reaching an accuracy of 0.7669000029563904


The training with 0.95 sparsity instead, as can be seen in the plot, is encountering overfitting on training data, because the validaiton accuracy is decreasing after 5 epochs.

On the other hand however, the convergence speed is much faster compared to 0.99 
and also the maximum reached accuracy is  higher than the one reached with 0.99 sparsity:
rounds:1 sparsity: 0.95 - max validation accuracy:	0.8084999918937683


----

Therefore we decided to go on and further experimenting by varying with the number of calibration rounds, adding also a sparsity of 0.90 in the param grid_dict,
and by limiting the number of epochs to 10 epochs.

(We are confident in decreasing the number of epochs not only because of the behavior we have just seen (faster convergence of sparse finetuned model)
but also because in the centralized baseline trainining, there was not so much difference in terms of validation accuracy when using 10 or 20 epochs
10 epochs:  0.7418000102043152
20: epochs: 0.7458999752998352
just (make percentage difference)% difference with respect to an almost doubled training time.
)

Setting: 
calibration rounds [1, 3, 5]
sparsity: [0.99, 0.95, 0.9]
epochs: 10


The results
[see plots: src/notebooks/experiments_results/centralized-baseline-model-editing/plots/baseline_model_editing_exp2_10epochs_*.pn
and experiment results: src/notebooks/experiments_results/centralized-baseline-model-editing/experiment2/*
]

show that the number of calibration rounds is not affecting in a meaningful way the final valdiation accuracy, when using the approximated fisher computastion of our setting.
In contract to the TA paper (Efficient model editing ...) which states instead that having a higher number of rounds improves the final performance.
Notice: the TA in his paper computes the exact FIM.

While instead the sparsity improved performance, being 0.90 better than > 0.95 better than > 0.99

Which demonstartes that sparse finetuning done by only finetuning the least important parameters according
to an approximated fisher computation is beneficial. This is important in practical scenarios where computing the extact 
FIM is too much expensive!

---

So next we will try to see if this is true also for FL iid and non-iid.