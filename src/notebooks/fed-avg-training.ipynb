{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ce451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def running_in_colab():\n",
    "    return 'google.colab' in sys.modules or os.path.exists('/content')\n",
    "\n",
    "branch = \"main\"\n",
    "username = \"giovanna-brod-zamojska\"\n",
    "repo = \"federated-learning-project\"\n",
    "\n",
    "is_private = True\n",
    "\n",
    "\n",
    "def clone_repo_if_needed(exists_ok: bool, username: str, repository: str, is_private: bool, branch: str = None):\n",
    "\n",
    "  colab_repo_path = f'/content/{repository}/'\n",
    "  \n",
    "  if running_in_colab():\n",
    "\n",
    "    if exists_ok and os.path.exists(colab_repo_path):\n",
    "        print(f\"Repository already exists at {colab_repo_path}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(colab_repo_path) or not exists_ok:\n",
    "\n",
    "        # Remove any existing repo\n",
    "        print(f\"Removing content of {colab_repo_path}\")\n",
    "        os.system(f\"rm -rf {colab_repo_path}\")\n",
    "        print(\"Current directory files and folders:\", os.system(\"ls\"))\n",
    "\n",
    "        print(\"Cloning GitHub repo...\")\n",
    "\n",
    "        if is_private:\n",
    "            # Clone private repository\n",
    "            # Clone the GitHub repo (only needed once, if not already cloned)\n",
    "            from getpass import getpass\n",
    "\n",
    "\n",
    "            # Prompt for GitHub token (ensure token has access to the repo)\n",
    "            token = getpass('Enter GitHub token: ')\n",
    "\n",
    "            if branch:\n",
    "              !git clone --branch {branch} https://{username}:{token}@github.com/{username}/{repo}.git\n",
    "            else: \n",
    "              !git clone https://{username}:{token}@github.com/{username}/{repo}.git\n",
    "\n",
    "        else:\n",
    "            # Clone public repository\n",
    "            if branch:\n",
    "              !git clone --branch {branch} https://github.com/{username}/{repo}.git\n",
    "            else:\n",
    "              !git clone https://github.com/{username}/{repo}.git\n",
    "\n",
    "\n",
    "    requirements_path = f\"{colab_repo_path}/colab-requirements.txt\"\n",
    "    !pip install -r \"$requirements_path\"\n",
    "\n",
    "  else:\n",
    "    print(\"Not running in Google Colab. Skipping repository cloning.\")#\n",
    "\n",
    "\n",
    "\n",
    "def setup_notebook(repo_root_name: str = \"federated-learning-project\"):\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    if running_in_colab():\n",
    "        print(\"Sys.path: \", sys.path)\n",
    "\n",
    "        colab_repo_path = f'/content/{repo_root_name}/'\n",
    "         # Add the repository root to sys.path so modules can be imported\n",
    "        if str(colab_repo_path) not in sys.path:\n",
    "            sys.path.insert(0, colab_repo_path)\n",
    "            print(f\"Added {colab_repo_path} to sys.path\")\n",
    "    else:\n",
    "      \n",
    "        notebook_dir = Path().absolute()\n",
    "        project_root = notebook_dir.parent.parent\n",
    "\n",
    "        # Add project root to Python path if not already present\n",
    "        if str(project_root) not in sys.path:\n",
    "            sys.path.insert(0, str(project_root))\n",
    "            print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "        \n",
    "clone_repo_if_needed(branch=branch, exists_ok=True, username=username, repository=repo, is_private=is_private)\n",
    "\n",
    "setup_notebook()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import flwr\n",
    "from flwr.client import ClientApp, NumPyClient\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner, DirichletPartitioner\n",
    "from flwr.common import Context\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "\n",
    "\n",
    "# --- Config ---\n",
    "\n",
    "# FedAvg with I.I.D. sharding\n",
    "\n",
    "# - K=100 (number of clients - FIXED)\n",
    "# - C=0.1 (fraction of clients used per round - FIXED)\n",
    "# - J=4 (number of local steps - FIXED)\n",
    "# - Nc={100} (number of labels for each client - FIXED) iid = each client receives an equal number of labels\n",
    "# - number of rounds -> \"proper\": up to you to define, based on convergence and time/compute budget\n",
    "#   comment: if C = 10%  and K=100 then \"number of rounds\" at least 10 where at each round we pass 10 different clients with respect to prev round (?)\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "NUM_CLIENTS = 100\n",
    "NC = 100\n",
    "NUM_ROUNDS = 2\n",
    "CLIENT_FRACTION_PER_ROUND = 0.1\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "PARTITION_TYPE = \"dirichlet\"  # \"iid\" or \"dirichlet\"\n",
    "\n",
    "LR = 0.05\n",
    "MOM = 0.9\n",
    "NUM_WORKERS = 4\n",
    "WD = 0.0005\n",
    "LOCAL_STEPS = 5\n",
    "\n",
    "# --- Dataset partitioning ---\n",
    "if PARTITION_TYPE == \"iid\":\n",
    "    partitioner = IidPartitioner(num_partitions=NUM_CLIENTS)\n",
    "else:\n",
    "    partitioner = DirichletPartitioner(\n",
    "        num_partitions=NUM_CLIENTS, alpha=0.5, partition_by=\"fine_label\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_datasets(partition_id: int):\n",
    "    fds = FederatedDataset(\n",
    "        dataset=\"uoft-cs/cifar100\",\n",
    "        partitioners={\"train\": partitioner},\n",
    "        seed=SEED,\n",
    "    )\n",
    "    partition = fds.load_partition(partition_id)\n",
    "    testset = fds.load_split(\"test\")\n",
    "\n",
    "    # Divide data on each node: 80% train, 20% test\n",
    "    partition_train_val = partition.train_test_split(test_size=0.2, seed=SEED)\n",
    "    num_classes = partition_train_val[\"train\"].features[\"fine_label\"].num_classes\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def apply_train_transform(batch):\n",
    "        batch[\"img\"] = [train_transform(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    def apply_test_transform(batch):\n",
    "        batch[\"img\"] = [test_transform(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    trainloader = DataLoader(\n",
    "        partition_train_val[\"train\"].with_transform(apply_train_transform),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valloader = DataLoader(\n",
    "        partition_train_val[\"test\"].with_transform(apply_test_transform),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    testloader = DataLoader(\n",
    "        testset.with_transform(apply_test_transform),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return trainloader, valloader, testloader, num_classes\n",
    "\n",
    "\n",
    "def load_dino_model(num_classes: int = 100):\n",
    "    # Load DINO ViT-S/16 model from torch hub\n",
    "\n",
    "    model = torch.hub.load(\"facebookresearch/dino:main\", \"dino_vits16\")\n",
    "    model.head = nn.Linear(384, num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"DINO ViT-S/16 model instantiated. Using device: {device}\")\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model, device\n",
    "\n",
    "\n",
    "def get_parameters(model) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(model, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(model.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "# Flower client implementation\n",
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cid: str,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        model,\n",
    "        device,\n",
    "    ):\n",
    "        self.cid = cid\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = SGD(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=LR,\n",
    "            momentum=MOM,\n",
    "            weight_decay=WD,\n",
    "        )\n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=LOCAL_STEPS)\n",
    "        print(f\"Client {cid} instantiated.\")\n",
    "\n",
    "    def get_parameters(self, config: Dict = None) -> List:\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return [val.cpu().numpy() for val in self.model.state_dict().values()]\n",
    "\n",
    "    def set_parameters(self, parameters: List, config: Dict = None) -> None:\n",
    "        print(f\"[Client {self.cid}] set_parameters, config: {config}\")\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        # state_dict = self.model.state_dict()\n",
    "        # for k, v in zip(state_dict.keys(), parameters):\n",
    "        #     state_dict[k] = torch.tensor(v)\n",
    "        # self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def fit(self, parameters: List, config: Dict = None) -> Tuple[List, int, Dict]:\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.train()\n",
    "\n",
    "        local_steps = LOCAL_STEPS  # Number of local SGD steps\n",
    "        steps_done = 0\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        # pbar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        while steps_done < local_steps:\n",
    "            for batch in self.train_loader:\n",
    "                x = batch[\"img\"]\n",
    "                y = batch[\"fine_label\"]\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "                features = self.model(x)\n",
    "                outputs = self.model.head(features)\n",
    "                loss = self.loss_fn(outputs, y)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                self.scheduler.step()\n",
    "\n",
    "                # Metrics and logging\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "                total += y.size(0)\n",
    "                correct += predicted.eq(y).sum().item()\n",
    "\n",
    "                accuracy = correct / total\n",
    "\n",
    "                steps_done += 1\n",
    "                # pbar.set_postfix({\"loss\": running_loss / total, \"accuracy\": accuracy})\n",
    "\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "        return self.get_parameters(), len(self.train_loader.dataset), {}\n",
    "\n",
    "    def evaluate(\n",
    "        self, parameters: List, config: Dict = None\n",
    "    ) -> Tuple[float, int, Dict]:\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                x = batch[\"img\"]\n",
    "                y = batch[\"fine_label\"]\n",
    "\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                outputs = self.model(x)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return float(accuracy), total, {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# --- Client factory ---\n",
    "\n",
    "\n",
    "def client_fn(context):\n",
    "\n",
    "    cid = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "\n",
    "    train_loader, val_loader, _, num_classes = load_datasets(partition_id=cid)\n",
    "\n",
    "    model, _ = load_dino_model(num_classes=num_classes)\n",
    "\n",
    "    return FlowerClient(\n",
    "        str(cid),\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model=model,\n",
    "        device=DEVICE,\n",
    "    ).to_client()\n",
    "\n",
    "\n",
    "# --- Server factory ---\n",
    "\n",
    "\n",
    "def server_fn(context: Context):\n",
    "\n",
    "    print(\"Calling server_fn\")\n",
    "\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=CLIENT_FRACTION_PER_ROUND,\n",
    "        fraction_evaluate=CLIENT_FRACTION_PER_ROUND,\n",
    "        min_fit_clients=2,\n",
    "        min_evaluate_clients=2,\n",
    "        min_available_clients=2,\n",
    "        evaluate_fn=None,\n",
    "    )\n",
    "\n",
    "    config = ServerConfig(num_rounds=NUM_ROUNDS)\n",
    "\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "# --- Server metrics ---\n",
    "# def weighted_average(metrics: List[Tuple[int, Dict[str, float]]]):\n",
    "#     total = sum([num for num, _ in metrics])\n",
    "#     acc = sum([num * m[\"accuracy\"] for num, m in metrics]) / total\n",
    "#     return {\"accuracy\": acc}\n",
    "\n",
    "\n",
    "# --- Run ---\n",
    "\n",
    "client_app = ClientApp(client_fn=client_fn)\n",
    "server_app = ServerApp(server_fn=server_fn)\n",
    "\n",
    "# Specify the resources each of your clients need\n",
    "# If set to none, by default, each client will be allocated 2x CPU and 0x GPUs\n",
    "backend_config = {\"client_resources\": None}\n",
    "if DEVICE.type == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_gpus\": 1, \"num_cpus\": 1}}\n",
    "\n",
    "run_simulation(\n",
    "    client_app=client_app,\n",
    "    server_app=server_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    "    verbose_logging=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
