{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "900ce451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Google Colab. Skipping repository cloning.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def running_in_colab():\n",
    "    return 'google.colab' in sys.modules or os.path.exists('/content')\n",
    "\n",
    "branch = \"main\"\n",
    "username = \"giovanna-brod-zamojska\"\n",
    "repo = \"federated-learning-project\"\n",
    "\n",
    "is_private = True\n",
    "\n",
    "\n",
    "def clone_repo_if_needed(exists_ok: bool, username: str, repository: str, is_private: bool, branch: str = None):\n",
    "\n",
    "  colab_repo_path = f'/content/{repository}/'\n",
    "  \n",
    "  if running_in_colab():\n",
    "\n",
    "    if exists_ok and os.path.exists(colab_repo_path):\n",
    "        print(f\"Repository already exists at {colab_repo_path}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(colab_repo_path) or not exists_ok:\n",
    "\n",
    "        # Remove any existing repo\n",
    "        print(f\"Removing content of {colab_repo_path}\")\n",
    "        os.system(f\"rm -rf {colab_repo_path}\")\n",
    "        print(\"Current directory files and folders:\", os.system(\"ls\"))\n",
    "\n",
    "        print(\"Cloning GitHub repo...\")\n",
    "\n",
    "        if is_private:\n",
    "            # Clone private repository\n",
    "            # Clone the GitHub repo (only needed once, if not already cloned)\n",
    "            from getpass import getpass\n",
    "\n",
    "\n",
    "            # Prompt for GitHub token (ensure token has access to the repo)\n",
    "            token = getpass('Enter GitHub token: ')\n",
    "\n",
    "            if branch:\n",
    "              !git clone --branch {branch} https://{username}:{token}@github.com/{username}/{repo}.git\n",
    "            else: \n",
    "              !git clone https://{username}:{token}@github.com/{username}/{repo}.git\n",
    "\n",
    "        else:\n",
    "            # Clone public repository\n",
    "            if branch:\n",
    "              !git clone --branch {branch} https://github.com/{username}/{repo}.git\n",
    "            else:\n",
    "              !git clone https://github.com/{username}/{repo}.git\n",
    "\n",
    "\n",
    "    requirements_path = f\"{colab_repo_path}/colab-requirements.txt\"\n",
    "    !pip install -r \"$requirements_path\"\n",
    "\n",
    "  else:\n",
    "    print(\"Not running in Google Colab. Skipping repository cloning.\")#\n",
    "\n",
    "\n",
    "\n",
    "def setup_notebook(repo_root_name: str = \"federated-learning-project\"):\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    if running_in_colab():\n",
    "        print(\"Sys.path: \", sys.path)\n",
    "\n",
    "        colab_repo_path = f'/content/{repo_root_name}/'\n",
    "         # Add the repository root to sys.path so modules can be imported\n",
    "        if str(colab_repo_path) not in sys.path:\n",
    "            sys.path.insert(0, colab_repo_path)\n",
    "            print(f\"Added {colab_repo_path} to sys.path\")\n",
    "    else:\n",
    "      \n",
    "        notebook_dir = Path().absolute()\n",
    "        project_root = notebook_dir.parent.parent\n",
    "\n",
    "        # Add project root to Python path if not already present\n",
    "        if str(project_root) not in sys.path:\n",
    "            sys.path.insert(0, str(project_root))\n",
    "            print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "        \n",
    "clone_repo_if_needed(branch=branch, exists_ok=True, username=username, repository=repo, is_private=is_private)\n",
    "\n",
    "setup_notebook()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting random seed to 42\n",
      "Training on cpu\n",
      "Flower 1.18.0 / PyTorch 2.2.2\n",
      "Dataset found at ./data. Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/giovanna/.cache/torch/hub/facebookresearch_dino_main\n",
      "\u001b[94mDEBUG 2025-05-28 22:40:47,384\u001b[0m:     Asyncio event loop already running.\n",
      "\u001b[94mDEBUG 2025-05-28 22:40:47,386\u001b[0m:     Logger propagate set to False\n",
      "\u001b[94mDEBUG 2025-05-28 22:40:47,391\u001b[0m:     Pre-registering run with id 7849729685942636560\n",
      "\u001b[94mDEBUG 2025-05-28 22:40:47,409\u001b[0m:     Using InMemoryState\n",
      "\u001b[94mDEBUG 2025-05-28 22:40:47,438\u001b[0m:     Using InMemoryState\n",
      "\u001b[92mINFO 2025-05-28 22:40:47,448\u001b[0m:      Starting Flower ServerApp, config: num_rounds=2, no round_timeout\n",
      "\u001b[94mDEBUG 2025-05-28 22:40:47,450\u001b[0m:     Using InMemoryState\n",
      "\u001b[92mINFO 2025-05-28 22:40:47,458\u001b[0m:      \n",
      "\u001b[94mDEBUG 2025-05-28 22:40:47,493\u001b[0m:     Registered 100 nodes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINO ViT-S/16 model instantied. Using device: cpu\n",
      "Replaced model.head (Identity) with Linear(384, 100)\n",
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Linear(in_features=384, out_features=100, bias=True)\n",
      ")\n",
      "ðŸ”’ Encoder parameters frozen, only head parameters are trainable.\n",
      "Checkpoint directory: ./checkpoints\n",
      "Trainer initialized.\n",
      "Calling server_fn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO 2025-05-28 22:40:47,506\u001b[0m:      [INIT]\n",
      "\u001b[94mDEBUG 2025-05-28 22:40:47,541\u001b[0m:     Supported backends: ['ray']\n",
      "\u001b[92mINFO 2025-05-28 22:40:47,547\u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[94mDEBUG 2025-05-28 22:40:47,577\u001b[0m:     Initialising: RayBackend\n",
      "\u001b[94mDEBUG 2025-05-28 22:40:47,616\u001b[0m:     Backend config: {'client_resources': None, 'init_args': {}, 'actor': {'tensorflow': 0}}\n"
     ]
    }
   ],
   "source": [
    "import flwr\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from flwr.server import ServerApp\n",
    "from flwr.client import ClientApp\n",
    "from flwr.simulation import run_simulation\n",
    "\n",
    "from src.classes.fedavg import server_fn, client_fn\n",
    "from src.classes.trainer import Trainer as FederatedTrainer\n",
    "from src.classes.dataset import CIFAR100Dataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    print(f\"Setting random seed to {seed}\")\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed = 42 \n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "LOCAL_EPOCHS = 1\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "\n",
    "NUM_CLIENTS = 100\n",
    "NC = 100\n",
    "NUM_ROUNDS = 2\n",
    "CLIENT_FRACTION_PER_ROUND = 0.1\n",
    "\n",
    "dataset = CIFAR100Dataset(num_clients=NUM_CLIENTS, nc=NC)\n",
    "\n",
    "trainer_config = {\n",
    "    \"seed\": seed,\n",
    "    \"lr\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"epochs\": LOCAL_EPOCHS,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "trainer = FederatedTrainer(\n",
    "    **trainer_config,\n",
    "    num_classes=dataset.get_num_labels(),\n",
    "    use_wandb=\"False\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "strategy_config = {\n",
    "    \"fraction_fit\": CLIENT_FRACTION_PER_ROUND,\n",
    "    \"fraction_eval\": CLIENT_FRACTION_PER_ROUND,\n",
    "}\n",
    "\n",
    "server_app = ServerApp(\n",
    "    server_fn=lambda ctx: server_fn(\n",
    "        ctx,\n",
    "        num_rounds=NUM_ROUNDS,\n",
    "        **strategy_config,\n",
    "    )\n",
    ")\n",
    "\n",
    "client_app = ClientApp(\n",
    "    client_fn=lambda ctx: client_fn(\n",
    "        ctx,\n",
    "        dataset,\n",
    "        split_type=\"iid\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Specify the resources each of your clients need\n",
    "# If set to none, by default, each client will be allocated 2x CPU and 0x GPUs\n",
    "\n",
    "backend_config = {\"client_resources\": None}\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_gpus\": 1}}\n",
    "\n",
    "run_simulation(\n",
    "    client_app=client_app,\n",
    "    server_app=server_app,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    "    verbose_logging=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
