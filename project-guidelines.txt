
Base model: DINO ViT-S/16
Dataset: CIFAR100

From guidelines:

TODO:
implement *checkpointing* and *experiment logging*. 
Colab will likely interrupt your experiments, so you must be ready to recover an interrupted run from a certain point in training 
when resources will be granted again. 

PROJECT

1. PRELIMINARIES

As you may have noticed, the CIFAR-100 dataset you downloaded from torchvision has not a validation split. 
Your first step will be to *split the original dataset such to have a validation set*, to be used for hyperparameter tuning.

In FL clients have their own disjoint set of training samples, so to simulate that setting you will need to *split the training 
set into K disjoint subsets*. 
In particular, to simulate statistical heterogeneity, for CIFAR-100 your code should implement the following splittings:
- *i.i.d. sharding*: each of K clients is given an approximately equal number of training samples uniformly distributed over the class labels
- *non-i.i.d. sharding*: each client is given an approximately equal number of training samples, belonging to Nc classes, where Nc is an 
    hyperparameter you will use to control the severity of the induced dataset heterogeneity. For example, if Nc=1, then each client has samples belonging to one class only.

2. CENTRALIZED BASELINE

Train your models on CIFAR-100 using the *SGDM optimizer*, taking care of *searching for the best hyparameters*. 
As a learning rate scheduler, we suggest you use the *cosine annealing scheduler*. 
*Report the plots of test loss and test accuracy*. 
How many epochs do you need? 
Which learning scheduler performs the best? 
Explore various solutions and compare your results

3. MODEL EDITING

    1. Check out the following code snippets to get used to computing parameter sensitivity 
    (i.e., the diagonal elements of the Fisher Information matrix) scores and calibrate gradient masks in multiple rounds:
    https://github.com/iurada/talos-task-arithmetic/blob/ae102473de0e57ebf625eca22e10781c371149ec/vision/pruners.py#L203
    https://github.com/iurada/talos-task-arithmetic/blob/main/vision/prune_finetune.py#L107 
    
    2. By extending SGDM, implement your SparseSGDM optimizer that accepts as input also a gradient mask to zero-out 
    the updates of the weights whose corresponding entry in the mask is zero. Here you can see the SGDM pseudo-code, for reference:
    https://pytorch.org/docs/stable/generated/torch.optim.SGD.html 
    More info can be found in [15] (HINT: see pseudo-code in the Appendix and github repo).

4. FL BASELINE
Implement the algorithm described in [10], fix K=100, C=0.1, adopt an iid sharding of the training set and fix J=4 the 
number of local steps. Run FedAvg on CIFAR-100 for a proper number of rounds (up to you to define, based on convergence 
and time/compute budget).

Simulate heterogeneous distributions
Fix K=100 and C=0.1, and simulate several non-iid shardings of the training set of CIFAR-100, by fixing the number of 
different labels clients have (Nc={1,5,10,50}). Then test the performance of FedAvg [10], comparing with the iid sharding, 
varying the number of local steps J={4,8,16}. When increasing the number of local steps, remember to scale accordingly 
the number of training rounds.
Is there a noticeable difference in performance? Motivate your findings.

Model Editing techniques in FL 
Sparse fine-tuning consists in updating only a selected subset of parameters by eg. masking the gradients during GD. 
This is typically done by:
(step 0.: Either train or obtain in closed-form eg. Ridge, a classifier locally which will be then kept frozen at all 
times after this step)
Calibrate a gradient mask, i.e., decide which weights will be updated (entry in the mask == 1) and which not (entry in the mask == 0). 
Calibrate the gradient mask by identifying in multiple rounds (see why in [15], Sec. 4.2.) the least-sensitive parameters 
(i.e. the weights with a sensitivity score lower than some user-defined threshold).
Perform fine-tuning by masking gradients with the calibrated masks. Use your implementation of SparseSGDM.

You should now experiment with: 
Sparsity ratio (defined by the sensitivity threshold chosen in step 1.)
Number of calibration rounds

5. PERSONAL CONTRIBUTION

Now that you have a clear understanding of challenges in FL, you can now explore a more specific problem. 
This part of your project is purposely weakly supervised, to let you come up with original ideas. Your can either propose:
An additional analysis of some aspects of federated learning (FL) or model editing not included in the above specifications
Original use of model editing techniques in federated learning, with the purpose of addressing the challenges outlined above
New model editing techniques inspired by the connections with FL and compatible with FL constraints
Try different gradient mask calibration rules (other than least-sensitive parameters). Specifically, you should try and 
compare with the previous results:

- Pick the most-sensitive weights (instead of the least-sensitive)
- Pick the lowest-magnitude weights (instead of the least-sensitive)
- Pick the highest-magnitude weights (instead of the least-sensitive)
- Pick random weights (instead of the least-sensitive)
